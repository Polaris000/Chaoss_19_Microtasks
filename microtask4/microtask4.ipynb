{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microtask 4\n",
    "\n",
    "## Aim:\n",
    "- Organize data for all repositories for the last three months\n",
    "- Present data as a csv and table\n",
    "- Order data based on the sum of commits, pull requests and issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Getting the data\n",
    "The data used for this microtask is the same as that used for the previous microtasks. Please check microtask 0 to see the cell output generated by running the commented script present two cells below this one. \n",
    "The cell below helps understand the data to be fetched, like the **owner**, the **repository names**, the **repository urls** and most importantly the **github authentication token**.  \n",
    "\n",
    "**Make sure to fill in your token for the `auth_token` variable in the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_url = \"https://github.com/\"  # the github url domain: used for generating repo_urls\n",
    "owner = \"progit\"\n",
    "repos_used = [\"progit2-ru\", \"progit2-zh\"]\n",
    "repo_urls = [github_url + owner + \"/\" + repo_used for repo_used in repos_used]\n",
    "auth_token = \"\" # Please enter your github token here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harnessing the power of jupyter notebooks\n",
    "The script in the cell below is a generalized way to create and populate a file named `progit.json`  using perceval.  \n",
    "\n",
    "The steps involved are simple: \n",
    "For each repository specified in the `repos_used` variable, fetch its git data, its pull_requests data and finally its issues data from the github api in that order and append them to `progit.json`. \n",
    "\n",
    "**Note**: it has been commented to prevent an accidental overwrite of the progit.json file, present in the parent directory of our present directory. To work on more recent data, or to perform an analysis on a completely different set of repositories, please uncomment the snippet below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for repo, repo_url in zip(repos_used, repo_urls):\n",
    "#     print(repo, repo_url)\n",
    "#     if repo == repos_used[0]:\n",
    "#         !perceval git --json-line $repo_url > ../progit.json\n",
    "\n",
    "#     else:\n",
    "#         !perceval git --json-line $repo_url >> ../progit.json\n",
    "\n",
    "#     !perceval github -t $auth_token --json-line --sleep-for-rate --category pull_request $owner $repo >> ../progit.json\n",
    "\n",
    "#     !perceval github -t $auth_token --json-line --sleep-for-rate --category issue $owner $repo >> ../progit.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import pprint\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "The `months` variable takes last 'x' number of months for which to calculate as its value. \n",
    "The two utility functions defined in the cells below are described here: \n",
    "\n",
    "**get_date_range**\n",
    "    \n",
    "  - parameters: int (number of months)\n",
    "  - returns: datetime object (start date)\n",
    "  \n",
    "  The function first creates a timedelta object based on the value of months. Here, a month is approximated to be 30 days. But this is an implementation detail and can be easily modified based on preference. Next, the function calculates the start date based on the timedelta object created. \n",
    "  \n",
    "\n",
    "**is_in_range**\n",
    "    \n",
    "  - parameters: str (date of item in string form as returned by perceval)\n",
    "                datetime object (start date as calculated by get_date_range)\n",
    "  - returns: Boolean (object lies in range or not)\n",
    "  \n",
    "   The code for this function is more or less the same one used in the other microtasks. The function quickly becomes complicated due to the fact that the date parameter for commits is different from that of issues or pull requests in the data fetched by perceval. \n",
    "   Once the item's date is converted to a comparable form, its compared with `start_date` which results in either True or False, depending on whether the creation date of that item happened in the last `num_months` months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_months = 3 # ask asked in problem statement\n",
    "\n",
    "\n",
    "def get_date_range(months):\n",
    "    current_date = datetime.datetime.now()\n",
    "    timediff = datetime.timedelta(hours=24*30*months)\n",
    "    start_date = current_date - timediff    \n",
    "    return start_date\n",
    "\n",
    "start_date = get_date_range(num_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_range(date_string, start_date):\n",
    "    try:\n",
    "        datetimestr =  datetime.datetime.strptime(date_string, \"%a %b %d %H:%M:%S %Y %z\").strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "    except ValueError as ve:\n",
    "        datetimestr =  datetime.datetime.strptime(date_string, \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    finally:\n",
    "        datetimeobj = datetime.datetime.strptime(datetimestr, \"%Y-%m-%d\")\n",
    "        if datetimeobj >= start_date:\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contents of github_data will once populated will look something like:\n",
    "\n",
    "```python\n",
    "    {\n",
    "        'commit': [commit1_dict, commit2_dict, ....], \n",
    "        'issue': [issue1_dict, issue2_dict, ....], \n",
    "        'pull_request': [pr1_dict, pr2_dict, ....], \n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_data =  {\n",
    "                \"commit\": [], \n",
    "                \"issue\": [], \n",
    "                \"pull_request\": []\n",
    "                }\n",
    "\n",
    "\n",
    "with open('../progit.json', 'r') as github_data_file:\n",
    "    for line in github_data_file:\n",
    "        data_line = json.loads(line)\n",
    "        category = data_line['category']\n",
    "        if category == \"commit\":\n",
    "            if is_in_range(data_line[\"data\"][\"CommitDate\"], start_date):\n",
    "                github_data[category].append(data_line)\n",
    "        else:\n",
    "            if is_in_range(data_line[\"data\"][\"created_at\"], start_date):\n",
    "                github_data[category].append(data_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total number of pull requests and issues\n",
    "\n",
    "Lets create a dictionary `repo_wise_issues_prs`, whose structure is shown below: \n",
    "```python\n",
    "    {\n",
    "        repo_url_1: {\"issue\": .., \"pull_request\": ..}, \n",
    "        repo_url_2: {\"issue\": .., \"pull_request\": ..},\n",
    "        repo_url_3: {\"issue\": .., \"pull_request\": ..}, \n",
    "        ..\n",
    "        .\n",
    "    }\n",
    "```\n",
    "The generic keys used allow this part of the script to work no matter which repositories or projects are used for the analysis.\n",
    "\n",
    "Looping through each issue and pull request in `github_data`, simply populate `repo_wise_issues_prs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"https://github.com/progit/progit2-ru\": {\n",
      "        \"issue\": 18,\n",
      "        \"pull_request\": 18,\n",
      "        \"commit\": 30\n",
      "    },\n",
      "    \"https://github.com/progit/progit2-zh\": {\n",
      "        \"issue\": 4,\n",
      "        \"pull_request\": 3,\n",
      "        \"commit\": 6\n",
      "    }\n",
      "}\n",
      "Total number of issues:  22\n",
      "Total number of pull requests:  21\n"
     ]
    }
   ],
   "source": [
    "repo_wise_data = {repo_url: {\"issue\": 0, \"pull_request\": 0, \"commit\": 0} for repo_url in repo_urls}\n",
    "total_issues = 0\n",
    "total_prs = 0\n",
    "total_commits = 0\n",
    "\n",
    "for elem in github_data['issue']:\n",
    "    repo_wise_data[elem['origin']]['issue'] += 1\n",
    "    total_issues += 1\n",
    "    \n",
    "for elem in github_data['pull_request']:\n",
    "    repo_wise_data[elem['origin']]['pull_request'] += 1\n",
    "    total_prs += 1\n",
    "    \n",
    "for elem in github_data['commit']:\n",
    "    repo_wise_data[elem['origin']]['commit'] += 1\n",
    "    total_commits += 1\n",
    "\n",
    "print(json.dumps(repo_wise_data, indent=4))\n",
    "print(\"Total number of issues: \", total_issues)\n",
    "print(\"Total number of pull requests: \", total_prs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing data as a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the cleaned data to a csv\n",
    "The following function takes a file path as a parameter and writes to that file the following: \n",
    "    The repositories for which data was fetched\n",
    "    number of commits, issues, and pull requests in the last `num_months` months\n",
    "    The total number of items (commits + issues + pull requests)\n",
    "    \n",
    "The actual process of writing to the csv is done with the help of the `csv` python package and specifically, `csv.writer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(file_path, repo_wise_data):\n",
    "    \n",
    "    repo_wise_data = dict(sorted(repo_wise_data.items(), key=lambda tup: (tup[1][\"issue\"]) + (tup[1][\"pull_request\"]) + (tup[1][\"commit\"])))\n",
    "    \n",
    "    with open(file_path, 'w') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile, delimiter=',')\n",
    "        \n",
    "        file_headers = [\"Repository\", \"Num_Commits\", \"Num_Issues\", \"Num_PRs\", \"Total\"]\n",
    "        csv_writer.writerow(file_headers)\n",
    "        \n",
    "        for key, val in repo_wise_data.items():\n",
    "            row = [key.replace(\"https://github.com\", ''),                        \\\n",
    "                 str(val[\"commit\"]) ,          \\\n",
    "                 str(val[\"issue\"]) ,           \\\n",
    "                 str(val[\"pull_request\"]) ,    \\\n",
    "                 str(val[\"commit\"]+ val[\"issue\"] + val[\"pull_request\"])       \n",
    "                  ]\n",
    "            csv_writer.writerow(x for x in row)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying a table based on the csv file\n",
    "The following function creates a table after the reading the csv file created above, allowing one to visualize the data stored in the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(file_path):\n",
    "    with open(file_path, 'r', ) as csvfile:\n",
    "        csv_reader = csv.reader(csvfile, delimiter=',')\n",
    "\n",
    "        for row in csv_reader:\n",
    "            for field in row:\n",
    "                print(\"%-18s\" %field, end=\"\\t\")\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository        \tNum_Commits       \tNum_Issues        \tNum_PRs           \tTotal             \t\n",
      "/progit/progit2-zh\t6                 \t4                 \t3                 \t13                \t\n",
      "/progit/progit2-ru\t30                \t18                \t18                \t66                \t\n"
     ]
    }
   ],
   "source": [
    "write_to_csv(\"../progit_last_3_months.csv\", repo_wise_data)\n",
    "create_table(\"../progit_last_3_months.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
