{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microtask 5\n",
    "\n",
    "## Aim:\n",
    "- Organize data for all repositories for the last three months\n",
    "- Present data as a csv and table\n",
    "- Order data based on the sum of commits, pull requests and issues\n",
    "- This is exactly the same as microtask 4, except that this must be done using pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Getting the data\n",
    "The data used for this microtask is the same as that used for the previous microtasks. Please check microtask 0 to see the cell output generated by running the commented script present two cells below this one. \n",
    "The cell below helps understand the data to be fetched, like the **owner**, the **repository names**, the **repository urls** and most importantly the **github authentication token**.  \n",
    "\n",
    "**Make sure to fill in your token for the `auth_token` variable in the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_url = \"https://github.com/\"  # the github url domain: used for generating repo_urls\n",
    "owner = \"atom\"\n",
    "repos_used = [\"language-java\", \"teletype\"]\n",
    "repo_urls = [github_url + owner + \"/\" + repo_used for repo_used in repos_used]\n",
    "auth_token = \"\" # Please enter your github token here\n",
    "file_name = owner + \".json\" # file to which perceval stores data (a ../ is automatically added)\n",
    "csv_name = owner + \"_last_3_months\" + \".csv\" # file to which csv data is written (a ../ is automatically added)Please enter your github token here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harnessing the power of jupyter notebooks\n",
    "The script in the cell below is a generalized way to create and populate a json file`  using perceval.  \n",
    "\n",
    "The steps involved are simple: \n",
    "For each repository specified in the `repos_used` variable, fetch its git data, its pull_requests data and finally its issues data from the github api in that order and append them to json file. \n",
    "\n",
    "**Note**: it has been commented to prevent an accidental overwrite of the json file, present in the parent directory of our present directory. To work on more recent data, or to perform an analysis on a completely different set of repositories, please uncomment the snippet below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for repo, repo_url in zip(repos_used, repo_urls):\n",
    "#     print(repo, repo_url)\n",
    "\n",
    "#     !perceval git --json-line $repo_url >> ../$file_name\n",
    "\n",
    "#     !perceval github -t $auth_token --json-line --sleep-for-rate --category pull_request $owner $repo >> ../$file_name\n",
    "\n",
    "#     !perceval github -t $auth_token --json-line --sleep-for-rate --category issue $owner $repo >> ../$file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "The `months` variable takes last 'x' number of months for which to calculate as its value. \n",
    "The two utility functions defined in the cells below are described here: \n",
    "\n",
    "**get_date_range**\n",
    "    \n",
    "  - parameters: int (number of months)\n",
    "  - returns: datetime object (start date)\n",
    "  \n",
    "  The function first creates a timedelta object based on the value of months. Here, a month is approximated to be 30 days. But this is an implementation detail and can be easily modified based on preference. Next, the function calculates the start date based on the timedelta object created. \n",
    "  \n",
    "\n",
    "**is_in_range**\n",
    "    \n",
    "  - parameters: str (date of item in string form as returned by perceval)\n",
    "                datetime object (start date as calculated by get_date_range)\n",
    "  - returns: Boolean (object lies in range or not)\n",
    "  \n",
    "   The code for this function is more or less the same one used in the other microtasks. The function quickly becomes complicated due to the fact that the date parameter for commits is different from that of issues or pull requests in the data fetched by perceval. \n",
    "   Once the item's date is converted to a comparable form, its compared with `start_date` which results in either True or False, depending on whether the creation date of that item happened in the last `num_months` months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_months = 3 # ask asked in problem statement\n",
    "\n",
    "\n",
    "def get_date_range(months):\n",
    "    current_date = datetime.datetime.now()\n",
    "    timediff = datetime.timedelta(hours=24*30*months)\n",
    "    start_date = current_date - timediff    \n",
    "    return start_date\n",
    "\n",
    "start_date = get_date_range(num_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_range(date_string, start_date):\n",
    "    try:\n",
    "        datetimestr =  datetime.datetime.strptime(date_string, \"%a %b %d %H:%M:%S %Y %z\").strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "    except ValueError as ve:\n",
    "        datetimestr =  datetime.datetime.strptime(date_string, \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    finally:\n",
    "        datetimeobj = datetime.datetime.strptime(datetimestr, \"%Y-%m-%d\")\n",
    "        if datetimeobj >= start_date:\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contents of github_data will once populated will look something like:\n",
    "\n",
    "```python\n",
    "    {\n",
    "        'commit': [commit1_dict, commit2_dict, ....], \n",
    "        'issue': [issue1_dict, issue2_dict, ....], \n",
    "        'pull_request': [pr1_dict, pr2_dict, ....], \n",
    "    }\n",
    "```\n",
    "\n",
    "Note: In the cell below, you might notice that there is an extra condition for when the category of the data is \"issue\". As mentioned in previous microtasks, the Github API assumes that all pull requests are issues as well, and hence, redundant pull request data is present in the \"issue\" category. The extra condition counters that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_data =  {\n",
    "                \"commit\": [], \n",
    "                \"issue\": [], \n",
    "                \"pull_request\": []\n",
    "                }\n",
    "\n",
    "\n",
    "with open('../' + file_name, 'r') as github_data_file:\n",
    "    for line in github_data_file:\n",
    "        data_line = json.loads(line)\n",
    "        category = data_line['category']\n",
    "        if category == \"commit\":\n",
    "            if is_in_range(data_line[\"data\"][\"CommitDate\"], start_date):\n",
    "                github_data[category].append(data_line)\n",
    "        else:\n",
    "            if is_in_range(data_line[\"data\"][\"created_at\"], start_date):\n",
    "                if (category == \"issue\" and \"pull_request\" not in data_line['data']) or category == \"pull_request\":\n",
    "                    github_data[category].append(data_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total number of pull requests and issues\n",
    "\n",
    "Lets create a dictionary `repo_wise_issues_prs`, whose structure is shown below: \n",
    "```python\n",
    "    {\n",
    "        repo_url_1: {\"issue\": .., \"pull_request\": ..}, \n",
    "        repo_url_2: {\"issue\": .., \"pull_request\": ..},\n",
    "        repo_url_3: {\"issue\": .., \"pull_request\": ..}, \n",
    "        ..\n",
    "        .\n",
    "    }\n",
    "```\n",
    "The generic keys used allow this part of the script to work no matter which repositories or projects are used for the analysis.\n",
    "\n",
    "Looping through each issue and pull request in `github_data`, simply populate `repo_wise_issues_prs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"https://github.com/atom/language-java\": {\n",
      "        \"issue\": 7,\n",
      "        \"pull_request\": 5,\n",
      "        \"commit\": 5\n",
      "    },\n",
      "    \"https://github.com/atom/teletype\": {\n",
      "        \"issue\": 6,\n",
      "        \"pull_request\": 2,\n",
      "        \"commit\": 1\n",
      "    }\n",
      "}\n",
      "Total number of issues:  13\n",
      "Total number of pull requests:  7\n"
     ]
    }
   ],
   "source": [
    "repo_wise_data = {repo_url: {\"issue\": 0, \"pull_request\": 0, \"commit\": 0} for repo_url in repo_urls}\n",
    "total_issues = 0\n",
    "total_prs = 0\n",
    "total_commits = 0\n",
    "\n",
    "for elem in github_data['issue']:\n",
    "    repo_wise_data[elem['origin']]['issue'] += 1\n",
    "    total_issues += 1\n",
    "    \n",
    "for elem in github_data['pull_request']:\n",
    "    repo_wise_data[elem['origin']]['pull_request'] += 1\n",
    "    total_prs += 1\n",
    "    \n",
    "for elem in github_data['commit']:\n",
    "    repo_wise_data[elem['origin']]['commit'] += 1\n",
    "    total_commits += 1\n",
    "\n",
    "print(json.dumps(repo_wise_data, indent=4))\n",
    "print(\"Total number of issues: \", total_issues)\n",
    "print(\"Total number of pull requests: \", total_prs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing data as a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the cleaned data to a csv\n",
    "The following function takes a file path and a dictionary containing the data as a parameter and writes to that file the following: \n",
    "    The repositories for which data was fetched\n",
    "    number of commits, issues, and pull requests in the last `num_months` months\n",
    "    The total number of items (commits + issues + pull requests)\n",
    "    \n",
    "The actual process of writing is done using `pandas.DataFrame.to_csv`. First the dictionary `repo_wise_data` is converted to a dataframe, which is later sorted based on the `Total` column, added later to the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(file_path, repo_wise_data):\n",
    "    \n",
    "#   repo_wise_data = dict((repo_wise_data[key.replace(\"https://github.com\", '')], value) for (key, value) in repo_wise_data.items())\n",
    "    \n",
    "    df = pd.DataFrame(repo_wise_data).transpose()\n",
    "    df.reset_index(level=0, inplace=True)\n",
    "    df[\"Total\"] = df[\"issue\"] + df[\"pull_request\"] + df[\"commit\"]\n",
    "    df = df.sort_values(\"Total\")\n",
    "    df.columns = [\"Repository\", \"Num_Commits\", \"Num_Issues\", \"Num_PRs\", \"Total\"]\n",
    "    df[\"Repository\"] = df[\"Repository\"].apply(lambda x: x.replace(\"https://github.com\", ''))\n",
    "    df = df.set_index('Repository')\n",
    "    df.to_csv(file_path)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_csv(\"../\" + csv_name, repo_wise_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying a table based on the csv file\n",
    "Instead of the csv.reader used in microtask1, the `pandas.read_csv()` is used to populate a dataframe, which is displayed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Repository</th>\n",
       "      <th>Num_Commits</th>\n",
       "      <th>Num_Issues</th>\n",
       "      <th>Num_PRs</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/atom/teletype</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/atom/language-java</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Repository  Num_Commits  Num_Issues  Num_PRs  Total\n",
       "0       /atom/teletype            1           6        2      9\n",
       "1  /atom/language-java            5           7        5     17"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"../\" + csv_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
